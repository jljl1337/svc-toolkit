{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Singing Voice Conversion Toolkit","text":"<p>A self-contained singing voice conversion application using the so-vits-svc architecture,  with Deep U-Net model for vocal separation feature and easy to use GUI.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<ol> <li> <p>Install Python (3.10 is recommended, but 3.10 - 3.11 should work)</p> </li> <li> <p>Install pipx</p> </li> <li> <p>Install the package by running this following terminal command if you only have one Python version installed:</p> </li> </ol> <pre><code>pipx install svc-toolkit\n</code></pre> <p>To install with a specific Python version, use the <code>--python</code> flag. For example, to install with Python 3.10:</p> <pre><code>pipx install svc-toolkit --python 3.10\n</code></pre> Using NVIDIA GPU <p>To use the package with NVIDIA GPU, you need to upgrade the following dependencies:</p> <pre><code>pipx inject svc-toolkit torch==2.1.1 torchaudio==2.1.1 --pip-args=\"-U\" --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>For CUDA version 11.*, you can change the <code>cu121</code> to <code>cu118</code>. So the command will be:</p> <pre><code>pipx inject svc-toolkit torch==2.1.1 torchaudio==2.1.1 --pip-args=\"-U\" --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>Note that AMD GPUs are not actively supported, but you can try using the package with the CPU version of PyTorch.</p> <p>For other installation options, see Installation.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#windows","title":"Windows","text":"<pre><code>svct.exe\n</code></pre>"},{"location":"#macoslinux","title":"macOS/Linux","text":"<pre><code>svct\n</code></pre> <p>For the detailed usage guide, see Usage.</p>"},{"location":"#development","title":"Development","text":"<p>For the detailed development guide, see Development.</p>"},{"location":"#about","title":"About","text":"<p>This project is the implementation of the final year project for the Bachelor of Science in Computer Science, Department of Computer Science, City University of Hong Kong, named \"Singing Voice Conversion from Fully Mixed Track with GUI\", with project code <code>23CS062</code>.</p>"},{"location":"development/","title":"Development","text":""},{"location":"development/#ide","title":"IDE","text":"<p>Visual Studio Code is recommended as the IDE for developing the package. You can install the recommended extensions by opening the workspace in the IDE. Useful commands are also provided as tasks in the <code>.vscode/tasks.json</code> file.</p> <p>Though, all IDEs that support Python development can be used, and those useful commands can be copied from the <code>.vscode/tasks.json</code> file to the IDE of your choice.</p>"},{"location":"development/#poetry","title":"Poetry","text":"<p>Poetry is used for managing the package dependencies, virtual environment, building and publishing the package. It is recommended to install Poetry globally on your system, instead of in the same virtual environment that installs the package. The recommended version is <code>1.8.2</code>, though newer versions should work.</p> <p>Using your own choice of virtual environment manager is also possible, but the steps to install for GPU support might be different.</p>"},{"location":"development/#development-environment","title":"Development Environment","text":"<p>To set up the development environment, follow these steps:</p> <ol> <li> <p>Install Python (3.10 is recommended, but 3.10 - 3.11 should work)</p> </li> <li> <p>Install Poetry</p> </li> <li> <p>Clone the repository and checkout a non-main branch</p> </li> <li> <p>Select the Python version to use with Poetry. For example, to use Python 3.10:</p> <pre><code>poetry env use path/to/python3.10\n</code></pre> </li> <li> <p>Install the package in editable mode</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Select the Python interpreter in the IDE to the virtual environment created by Poetry</p> </li> <li> <p>Upgrade the dependencies if you want to develop with NVIDIA GPU</p> <p>Activate the virtual environment created by Poetry if it is not activated:</p> <pre><code>poetry shell\n</code></pre> <p>Then, run the following command:</p> <pre><code>pip install -U torch==2.1.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>For CUDA version 11.*, you can change the <code>cu121</code> to <code>cu118</code>. So the command will be:</p> <pre><code>pip install -U torch==2.1.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n</code></pre> </li> <li> <p>Good to go!</p> </li> </ol>"},{"location":"development/#testing","title":"Testing","text":"<p>To run the tests, run the following command:</p> <pre><code>poetry run pytest\n</code></pre> <p>If the poetry environment is activated, you can just run <code>pytest</code> straight away:</p> <pre><code>pytest\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#option-1-pipx","title":"Option 1: pipx","text":"<ol> <li> <p>Install Python (3.10 is recommended, but 3.10 - 3.11 should work)</p> </li> <li> <p>Install pipx</p> </li> <li> <p>Install the package by running this following terminal command if you only have one Python version installed:</p> </li> </ol> <pre><code>pipx install svc-toolkit\n</code></pre> <p>To install with a specific Python version, use the <code>--python</code> flag. For example, to install with Python 3.10:</p> <pre><code>pipx install svc-toolkit --python 3.10\n</code></pre> Using NVIDIA GPU <p>To use the package with NVIDIA GPU, you need to upgrade the following dependencies:</p> <pre><code>pipx inject svc-toolkit torch==2.1.1 torchaudio==2.1.1 --pip-args=\"-U\" --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>For CUDA version 11.*, you can change the <code>cu121</code> to <code>cu118</code>. So the command will be:</p> <pre><code>pipx inject svc-toolkit torch==2.1.1 torchaudio==2.1.1 --pip-args=\"-U\" --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>For usage, see here</p>"},{"location":"installation/#option-2-virtual-environment","title":"Option 2: Virtual Environment","text":"<p>Any virtual environment manager can be used to create a virtual environment for the package. Here is an example using <code>miniconda</code>:</p> <ol> <li> <p>Install miniconda</p> </li> <li> <p>Create a new environment:</p> <pre><code>conda create -n svc-venv python=3.10\n</code></pre> </li> <li> <p>Activate the environment:</p> <pre><code>conda activate svc-venv\n</code></pre> </li> <li> <p>Install the package:</p> <pre><code>pip install svc-toolkit\n</code></pre> </li> </ol> <p>Note that <code>svc-venv</code> is the environment name, you can change it to any name you like.</p> Using NVIDIA GPU <p>To use the package with NVIDIA GPU, you need to upgrade the following dependencies:</p> <pre><code>pip install -U torch==2.1.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>For CUDA version 11.*, you can change the <code>cu121</code> to <code>cu118</code>. So the command will be:</p> <pre><code>pip install -U torch==2.1.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>For usage, see here</p>"},{"location":"installation/#what-is-the-difference-between-these-two-options-which-one-should-i-choose","title":"What is the difference between these two options? Which one should I choose?","text":"<p>For those who are familiar with Python, you may have used <code>virtualenv</code> or <code>venv</code> to create isolated environments for your Python projects. This is adopted to avoid the same dependency with different version conflicts between different projects.</p> <p>pipx is just built on top of <code>venv</code>, what it does is to create a virtual environment for each package you install, and install the package in that virtual environment. It is usually used for installing packages with entry point(s), like <code>svct</code> in this case.</p> <p>One of the advantages of using <code>pipx</code> is that the entry point of the package is available in your shell, so you can run the package directly from the terminal without activating the virtual environment.</p> <p>However, if you are already familiar with using virtual environment, and you want to manage the environment yourself, you can use your preferred tools to create one and install the package in that environment.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#reinstalling-the-package","title":"Reinstalling the package","text":"<p>If the package is not working as expected, try uninstalling and installing the package again.</p> <p>If the package is installed using <code>pipx</code>, uninstall the package using this command:</p> <pre><code>pipx uninstall svc-toolkit\n</code></pre> <p>If the package is installed using virtual environment, uninstall the package by  deactivating the virtual environment and removing it.</p> <p>Then, install the package again. For installation instructions, see here.</p>"},{"location":"troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p>If the GPU is not detected, make sure that the latest version of the GPU driver is installed and the correct version of PyTorch is installed. You may uninstall the package and reinstall it with the correct version of PyTorch.</p>"},{"location":"troubleshooting/#poetry-install-hangs","title":"Poetry install hangs","text":"<p>If <code>poetry install</code> hangs, try running the command with the <code>-vvv</code> flag to see what's happening.</p> <pre><code>poetry install -vvv\n</code></pre> <p>If it stop at something like this:</p> <pre><code>[keyring.backend] Loading SecretService\n[keyring.backend] Loading Windows\n[keyring.backend] Loading chainer\n[keyring.backend] Loading libsecret\n[keyring.backend] Loading macOS\n</code></pre> <p>Then you may run this command:</p> <pre><code>poetry config keyring.enabled false\n</code></pre> <p>source</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#starting-the-application","title":"Starting the Application","text":"<p>Please install the package first before using it. For installation instructions, see here.</p> <p>To start the application, run the following command in the terminal:</p> <p>Windows:</p> <pre><code>svct.exe\n</code></pre> <p>macOS/Linux:</p> <pre><code>svct\n</code></pre> <p>A loading screen will appear, and the application will start after a few seconds. The screen is different depending on the operating system. In this documentation, the macOS version is used. Though the appearance is different, the functionality is the same on all platforms.</p> <p></p>"},{"location":"usage/#vocal-separation-tab","title":"Vocal Separation Tab","text":""},{"location":"usage/#files","title":"Files","text":"<p>In this tab, you can separate the vocals from the fully mixed track. The input file and the output directory has to be specified. The output directory is where the separated vocals will be saved.</p>"},{"location":"usage/#options","title":"Options","text":"<p>At least one outputting file has to be selected before starting the separation. Model of different size can be selected, the larger the model, the better the separation quality. However, the larger model will take more time to process.</p>"},{"location":"usage/#device-and-precision","title":"Device and Precision","text":"<p>The device can be selected to be either CPU or GPU. If the device is set to GPU, the model will be loaded to the GPU and the separation will be done on the GPU. It is generally faster to use the GPU, so it is recommended to use the GPU if available. As for macOS, Metal (MPS) can be selected as the device for faster processing.</p> <p>For both Metal (MPS) on macOS and NVIDIA GPU that is pre-Ampere architecture, the BFloat16 option is not supported. The BFloat16 option is only available for NVIDIA GPU with Ampere architecture or newer. As an alternative, the Float32 option can be used, though the quality might be slightly worse, or CPU can be used with the same quality, but slower processing.</p>"},{"location":"usage/#start-separation","title":"Start Separation","text":"<p>After selecting the input file, output directory, and the options, click the \"Start Separation\" button to start the separation process. The progress bar will show the progress of the separation. The time taken for the separation depends on the size of the input file and the selected model.</p> <p>Note that the application will download the model if it is not found in the cache, so the first time the model is used, it will take longer to start the separation, especially if the model is large.</p>"},{"location":"usage/#training-tab","title":"Training Tab","text":""},{"location":"usage/#preprocessing","title":"Preprocessing","text":"<p>Before the training, the dataset has to be preprocessed. The dataset should be in the format of a directory containing the audio files. The dataset directory has to be specified, and the output directory for the preprocessed dataset has to be specified as well.</p> <p>If the audio files are very long, the audio files can be split into smaller segments by checking the \"Split Audio Files\" option.</p> <p>The preprocessing can be started by clicking the \"Start Preprocessing\" button. The preprocessing will take some time depending on the size of the dataset. When the preprocessing is running, there will be an animation indicating that the preprocessing is running.</p>"},{"location":"usage/#training","title":"Training","text":"<p>After the dataset is preprocessed, the training can be started. The path to the folder of the outputting model and the path to the outputting config file has to be specified. The config file is named <code>config.json</code>, containing the training configuration, that is generated when the dataset is preprocessed.</p> <p>The training can be started by clicking the \"Start Training\" button. The training will take some time depending on the size of the dataset and the selected model. When the training is running, there will be an animation indicating that the training is running.</p>"},{"location":"usage/#conversion-tab","title":"Conversion Tab","text":""},{"location":"usage/#files_1","title":"Files","text":"<p>In this tab, you can convert the vocals to a different singer. The input, output, model, and config file has to be specified. The model file should be started with the letter <code>G</code>, that is something like <code>G_XXX.pth</code>, where <code>XXX</code> is the number of epochs.</p>"},{"location":"usage/#options_1","title":"Options","text":"<p>After selecting the config file, the speaker dropdown will be populated with the speakers in the config file. The pitch of the converted vocal can be adjusted by using the pitch slider. If the converted vocal is a speaking voice, the \"Auto Predict F0\" option can be checked to automatically predict the pitch of the speaking voice.</p>"},{"location":"usage/#advanced-settings","title":"Advanced Settings","text":"<p>The advanced settings can be used to adjust the conversion settings. Do note that the advanced settings are for advanced users, and the default settings should work for most cases.</p>"},{"location":"usage/#start-conversion","title":"Start Conversion","text":"<p>The conversion can be started by clicking the \"Start Conversion\" button. Similar to the training, an animation will be shown when the conversion is running. The time taken for the conversion depends on the size of the input file.</p>"},{"location":"usage/#mixing-tab","title":"Mixing Tab","text":""},{"location":"usage/#files_2","title":"Files","text":"<p>In this tab, you can mix the converted vocal with the instrumental. The source 1 file, source 2 file, and the output file paths have to be specified. Do note that this tab can also be used to mix any two audio files, as long as the audio files have the exact same sample rate, number of channels, and length.</p>"},{"location":"usage/#options_2","title":"Options","text":"<p>The volume ratio of source 1 can be adjusted by using the slider. The volume ratio is between 0 and 1. Since when mixing two audio files, the volume might be too loud, the normalization option is checked by default to normalize the volume of the mixed audio.</p>"},{"location":"usage/#vocal-separation-model-training-and-evaluation","title":"Vocal Separation Model Training and Evaluation","text":"<p>Important: This section is for advanced users who want to train the vocal separation model from scratch. The training and evaluation is done in the terminal. The interface is command-line based only, though a GUI version might be added in the future.</p> <p>Note that installing the package from source is recommended for these features, as it is easier to locate the manifest and model files. For installation instructions, see here.</p> <p>The current models are trained using MUSDB18-HQ and MoisesDB datasets, with a total of around 18 hours of training data. The resulting models are able to separate the vocals from the fully mixed track, but if you have a larger dataset, you can train the model and evaluate the model using the provided scripts. You are strongly encouraged to upload the trained model to Hugging Face for others to use, if possible.</p>"},{"location":"usage/#preprocessing_1","title":"Preprocessing","text":"<p>Important: Note that this feature is available only when installing the package from source. Please see Development for more information.</p> <p>This entry point is used to generate the csv files from the downloaded MUDB18-HQ and MoisesDB datasets. If you are using your own dataset, you should generate the csv files in the format specified in the next section.</p> <p>To preprocess the dataset, run the following command:</p> <p>Windows:</p> <pre><code>vs-preprocess.exe\n</code></pre> <p>macOS/Linux:</p> <pre><code>vs-preprocess\n</code></pre> <p>The help message is as follows:</p> <pre><code>usage: vs-preprocess [-h] [-o CSV_OUTPUT_DIR] [-v VAL_SIZE] [-s STEM] -m MUSDB_DIR [-M MOISESDB_DIR] -w MOISESDB_WAV_DIR\n\nPreprocess the dataset(s).\n\noptions:\n  -h, --help            show this help message and exit\n  -o CSV_OUTPUT_DIR, --csv_output_dir CSV_OUTPUT_DIR\n                        CSV Output directory (default: ./input_csv)\n  -v VAL_SIZE, --val_size VAL_SIZE\n                        Validation size (default: 0.2)\n  -s STEM, --stem STEM  Stem to preprocess (default: vocals)\n  -m MUSDB_DIR, --musdb_dir MUSDB_DIR\n                        Path to the MUSDB18 directory (required)\n  -M MOISESDB_DIR, --moisesdb_dir MOISESDB_DIR\n                        Path to the MoisesDB directory (optional)\n  -w MOISESDB_WAV_DIR, --moisesdb_wav_dir MOISESDB_WAV_DIR\n                        Path to the MoisesDB wav directory (required)\n</code></pre> <p>The <code>MUSDB_DIR</code> is the path to the MUSDB18-HQ dataset, which should contain the <code>train</code> and <code>test</code> directories. The <code>MOISESDB_DIR</code> is the path to the MoisesDB dataset, which should contain the <code>moisesd_v0.1</code> directory. It has to be specified if the mixed stem audio files are not yet generated. The <code>MOISESDB_WAV_DIR</code> is the path to the MoisesDB wav directory, which is the output directory of the MoisesDB mixed stem audio files, and the input directory of the MoisesDB CSV files.</p> <p>The <code>MOISESDB_DIR</code> has to be specified for the first time the MoisesDB dataset is used to generate the mixed stem audio files in a similar format to the MUSDB18-HQ dataset, and can be omitted for subsequent runs.</p>"},{"location":"usage/#training_1","title":"Training","text":"<p>This entry point is used to train the vocal separation model. The training is done using PyTorch Lightning, and the model is saved in the output directory specified.</p> <p>To train the model, run the following command:</p> <p>Windows:</p> <pre><code>vs-train.exe\n</code></pre> <p>macOS/Linux:</p> <pre><code>vs-train\n</code></pre> <p>The help message is as follows:</p> <pre><code>usage: vs-train [-h] -t TRAIN_CSV -v VAL_CSV [-e EXPERIMENT] [-m MODEL_LOG_DIR] [-c CONFIG]\n\nTrain a separation model.\n\noptions:\n  -h, --help            show this help message and exit\n  -t TRAIN_CSV, --train_csv TRAIN_CSV\n                        Path to the training csv file (required)\n  -v VAL_CSV, --val_csv VAL_CSV\n                        Path to the validation csv file (required)\n  -e EXPERIMENT, --experiment EXPERIMENT\n                        Name of the experiment (default: exp)\n  -m MODEL_LOG_DIR, --model_log_dir MODEL_LOG_DIR\n                        Path to the model log directory (default: ./model_log/)\n  -c CONFIG, --config CONFIG\n                        Path to the config file (default: ./config.yml)\n</code></pre> <p>Note that the experiment name is used to differentiate between different experiments, and a new directory will be created in the <code>MODEL_LOG_DIR</code> with the experiment name. The experiment name can also be used to create nested directories, for example, <code>exp1/config1</code>.</p> <p>The CSV files should have three columns in total, all of which are strings, they are as follows:</p> <ul> <li><code>song</code></li> <li><code>mixture_path</code></li> <li><code>stem_path</code></li> </ul> <p>The <code>song</code> column is the name of the song, usually follow the format of <code>artist - title</code>. The <code>mixture_path</code> column is the path to the mixture audio file, and the <code>stem_path</code> column is that to the stem audio file. Any CSV file that follows this format can be used for training, even if it is not generated using the preprocessing script.</p> <p>The config file is a YAML file containing the training configuration. The format of the config file should follow that of this example here. There are a few parameters that are worth noting: <code>resume_path</code> is the path to the directory of the experiment to resume training from, <code>expand_factor</code> is the factor to expand the dataset by, which should be around the length of a patch/segment. <code>deeper</code> is the flag to train the large model.</p> <p>Some of the parameters in the config file have fixed possible values, they and their possible values are as follows:</p> <ul> <li><code>precision</code>: <code>bf16</code> or <code>fp32</code></li> <li><code>neglect_frequency</code>: <code>nyquist</code> or <code>zero</code></li> <li><code>optimizer</code>: <code>adam</code> or <code>adamw</code></li> </ul>"},{"location":"usage/#evaluation","title":"Evaluation","text":"<p>This entry point is used to evaluate the vocal separation model. The evaluation is done on the files with their paths stored in a CSV file, and the model is loaded from the output directory</p> <p>To evaluate the model, run the following command:</p> <p>Windows:</p> <pre><code>vs-eval.exe\n</code></pre> <p>macOS/Linux:</p> <pre><code>vs-eval\n</code></pre> <p>The help message is as follows:</p> <pre><code>usage: vs-eval [-h] -m MODEL_DIR -t TEST_CSV [-p {bf16,fp32}] [-l | --last | --no-last]\n\nEvaluate a separation model.\n\noptions:\n  -h, --help            show this help message and exit\n  -m MODEL_DIR, --model_dir MODEL_DIR\n                        Path to the model directory (required)\n  -t TEST_CSV, --test_csv TEST_CSV\n                        Path to the test csv file (required)\n  -p {bf16,fp32}, --precision {bf16,fp32}\n                        Precision (default: bf16)\n  -l, --last, --no-last\n                        Use the last model\n</code></pre> <p>The format of the CSV file is the same as the training CSV file, please refer to the training section for more information. The precision can be selected to be either BFloat 16 (<code>bf16</code>) or Float 32 (<code>32</code>), with BFloat 16 being the default. The last model can be used by specifying the <code>--last</code> flag, or otherwise, the best model (i.e. the one with the lowest validation loss) will be used.</p> <p>Note that the results will be saved in the same directory as the model directory. The results include the SDR, SI-SDR, NSDR, and NSI-SDR scores. The scores of each song will be saved in a CSV file, and the summary of the scores will be saved in another CSV file. A box plot of the scores will also be saved in the same directory.</p>"}]}